{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzZHcGDx9SPbcVOC2Cw5K4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ish02K/PPD-for-NN/blob/main/Assignment_27_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSDdemoaeT6R"
      },
      "outputs": [],
      "source": [
        "# PRE-PROCESSING DATA FOR NEURAL NETWORKS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize each feature using scikit-learn’s StandardScaler"
      ],
      "metadata": {
        "id": "BMc1g8jJebI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "# Create feature\n",
        "features = np.array([[-100.1, 3240.1],\n",
        "                     [-200.2, -234.1],\n",
        "                     [5000.5, 150.1],\n",
        "                     [6000.6, -125.1],\n",
        "                     [9000.9, -673.1]])\n",
        "\n",
        "# Create scaler\n",
        "scaler = preprocessing.StandardScaler()\n",
        "\n",
        "# Transform the feature\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "\n",
        "# Show feature\n",
        "features_standardized"
      ],
      "metadata": {
        "id": "v9UrtrUtefn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171bc0ef-f452-4e88-e23e-65d6cc71298e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.12541308,  1.96429418],\n",
              "       [-1.15329466, -0.50068741],\n",
              "       [ 0.29529406, -0.22809346],\n",
              "       [ 0.57385917, -0.42335076],\n",
              "       [ 1.40955451, -0.81216255]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print mean and standard deviation\n",
        "print(\"Mean:\", round(features_standardized[:,0].mean()))\n",
        "print('\"Standard deviation:\", features_standardized[:,0].std()')"
      ],
      "metadata": {
        "id": "E10bK6O2l5eD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1146d81-7a61-4e7b-be9c-91d2174b9ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 0\n",
            "\"Standard deviation:\", features_standardized[:,0].std()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DESIGNING A NEURAL NETWORK"
      ],
      "metadata": {
        "id": "mLMrJ8Bzl-1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Keras’ Sequential model"
      ],
      "metadata": {
        "id": "ixSjARuzmFe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Start neural network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
        "\n",
        "# Add fully connected layer with a sigmoid activation function\n",
        "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile neural network\n",
        "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
        "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
        "                metrics=[\"accuracy\"]) # Accuracy performance metric\n"
      ],
      "metadata": {
        "id": "N4cs3Hs3mKwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING A BINARY CLASSIFIER"
      ],
      "metadata": {
        "id": "qU1pjTHJmZsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Keras to construct a feedforward neural network"
      ],
      "metadata": {
        "id": "StXuyNefmi8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set the number of features we want\n",
        "number_of_features = 1000\n",
        "\n",
        "# Load data and target vector from movie review data\n",
        "(data_train, target_train), (data_test, target_test) = imdb.load_data( num_words=number_of_features)\n",
        "\n",
        "# Convert movie review data to one-hot encoded feature matrix\n",
        "tokenizer = Tokenizer(num_words=number_of_features)\n",
        "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
        "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
        "\n",
        "# Start neural network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
        "\n",
        "# Add fully connected layer with a sigmoid activation function\n",
        "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile neural network\n",
        "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
        "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
        "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
        "\n",
        "# Train neural network\n",
        "history = network.fit(features_train, # Features\n",
        "                      target_train, # Target vector\n",
        "                      epochs=3, # Number of epochs\n",
        "                      verbose=1, # Print description after each epoch\n",
        "                      batch_size=100, # Number of observations per batch\n",
        "                      validation_data=(features_test, target_test)) # Test data"
      ],
      "metadata": {
        "id": "KgMs1JNgmoF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ce1999-6c53-4728-d52d-ac9eaf70e705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.4115 - accuracy: 0.8152 - val_loss: 0.3390 - val_accuracy: 0.8548\n",
            "Epoch 2/3\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.3247 - accuracy: 0.8657 - val_loss: 0.3265 - val_accuracy: 0.8595\n",
            "Epoch 3/3\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3135 - accuracy: 0.8686 - val_loss: 0.3355 - val_accuracy: 0.8560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View shape of feature matrix\n",
        "features_train.shape"
      ],
      "metadata": {
        "id": "xQoy0Qp-nRUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "525c7618-95e2-4318-9712-84f04c040839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING MULTI-CLASS CLASSIFIER"
      ],
      "metadata": {
        "id": "etZWicmwnXRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from keras.datasets import reuters\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set the number of features we want\n",
        "number_of_features = 5000\n",
        "\n",
        "# Load feature and target data\n",
        "data = reuters.load_data(num_words=number_of_features)\n",
        "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
        "\n",
        "# Convert feature data to a one-hot encoded feature matrix\n",
        "tokenizer = Tokenizer(num_words=number_of_features)\n",
        "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
        "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
        "\n",
        "# One-hot encode target vector to create a target matrix\n",
        "target_train = to_categorical(target_vector_train)\n",
        "target_test = to_categorical(target_vector_test)\n",
        "\n",
        "# Start neural network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=100,\n",
        "            activation=\"relu\",\n",
        "            input_shape=(number_of_features,)))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
        "\n",
        "# Add fully connected layer with a softmax activation function\n",
        "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
        "\n",
        "# Compile neural network\n",
        "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
        "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
        "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
        "# Train neural network\n",
        "history = network.fit(features_train, # Features\n",
        "                      target_train, # Target\n",
        "                      epochs=3, # Three epochs\n",
        "                      verbose=0, # No output\n",
        "                      batch_size=100, # Number of observations per batch\n",
        "                      validation_data=(features_test, target_test)) # Test data"
      ],
      "metadata": {
        "id": "rLg0ui7cnh7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c084552e-0147-4e12-956b-1ff8f1e371bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "2121728/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View target matrix\n",
        "target_train"
      ],
      "metadata": {
        "id": "2voAZgWwoKut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1cfaedd-f7e3-4a90-9dd5-f8ce44295504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING A REGRESSOR"
      ],
      "metadata": {
        "id": "dcLjWoZ4oQN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate features matrix and target vector\n",
        "features, target = make_regression(n_samples = 10000,\n",
        "                                   n_features = 3,\n",
        "                                   n_informative = 3,\n",
        "                                   n_targets = 1,\n",
        "                                   noise = 0.0,\n",
        "                                   random_state = 0)\n",
        "\n",
        "# Divide our data into training and test sets\n",
        "features_train, features_test, target_train, target_test = train_test_split(\n",
        "features, target, test_size=0.33, random_state=0)\n",
        "\n",
        "# Start neural network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=32,activation=\"relu\",input_shape=(features_train.shape[1],)))\n",
        " \n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=32, activation=\"relu\"))\n",
        "\n",
        "# Add fully connected layer with no activation function\n",
        "network.add(layers.Dense(units=1))\n",
        "\n",
        "# Compile neural network\n",
        "network.compile(loss=\"mse\", # Mean squared error\n",
        "                optimizer=\"RMSprop\", # Optimization algorithm\n",
        "                metrics=[\"mse\"]) # Mean squared error\n",
        "\n",
        "# Train neural network\n",
        "history = network.fit(features_train, # Features\n",
        "                      target_train, # Target vector\n",
        "                      epochs=10, # Number of epochs\n",
        "                      verbose=0, # No output\n",
        "                      batch_size=100, # Number of observations per batch\n",
        "                      validation_data=(features_test, target_test)) # Test data"
      ],
      "metadata": {
        "id": "Q2x6LAc8oWGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAKING PREDICTIONS"
      ],
      "metadata": {
        "id": "XCxMfOpeo6xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set the number of features we want\n",
        "number_of_features = 10000\n",
        "\n",
        "# Load data and target vector from IMDB movie data\n",
        "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words=number_of_features)\n",
        " \n",
        "# Convert IMDB data to a one-hot encoded feature matrix\n",
        "tokenizer = Tokenizer(num_words=number_of_features)\n",
        "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
        "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
        "\n",
        "# Start neural network\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
        "\n",
        "# Add fully connected layer with a sigmoid activation function\n",
        "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile neural network\n",
        "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
        "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
        "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
        "\n",
        "# Train neural network\n",
        "history = network.fit(features_train, # Features\n",
        "                      target_train, # Target vector\n",
        "                      epochs=3, # Number of epochs\n",
        "                      verbose=0, # No output\n",
        "                      batch_size=100, # Number of observations per batch\n",
        "                      validation_data=(features_test, target_test)) # Test data\n",
        "\n",
        "# Predict classes of test set\n",
        "predicted_target = network.predict(features_test)"
      ],
      "metadata": {
        "id": "yTfSkySuo_my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the probability the first observation is class 1\n",
        "predicted_target[0]"
      ],
      "metadata": {
        "id": "ValS8P5-png6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb420fe-4449-4e32-ef2a-86c57a249e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.14639106], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j3jakeiXpoMT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}